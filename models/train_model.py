import json
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import save_model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from mlp_model import build_mlp_model

# Charger les bases de donn√©es de mots-cl√©s
def load_keywords(file_path):
    """
    Charge la base de donn√©es de mots-cl√©s et extrait les termes sous forme de liste de mots.
    """
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    words = []
    severity_map = {}  # Associe chaque mot-cl√© √† sa s√©v√©rit√©

    for item in data["keywords"]:
        word = item["word"]
        synonyms = item["synonyms"]
        severity = item["severity"]

        # V√©rifier si "word" est une cha√Æne et non un dictionnaire
        if isinstance(word, str):
            words.append(word)
            severity_map[word] = severity

        # V√©rifier si "synonyms" est une liste de cha√Ænes et non un dict
        if isinstance(synonyms, list):
            for synonym in synonyms:
                if isinstance(synonym, str) and synonym.strip():  # Exclure les cha√Ænes vides
                    words.append(synonym)
                    severity_map[synonym] = severity

    return words, severity_map

# Charger les mots-cl√©s SCA et non-SCA
sca_words, severity_sca = load_keywords("data/sca_words.json")
non_sca_words, severity_non_sca = load_keywords("data/non_sca_words.json")

# Fusionner les bases de donn√©es
all_words = sca_words + non_sca_words
severity_map = {**severity_sca, **severity_non_sca}

# ‚úÖ V√©rification apr√®s extraction
print(f"üîç Nombre total de mots-cl√©s : {len(all_words)}")
print(f"üîç Exemple de mots-cl√©s : {all_words[:5]}")
print(f"üîç Exemple de s√©v√©rit√© : {list(severity_map.items())[:5]}")

# V√©rification finale
if any(isinstance(item, dict) for item in all_words):
    raise TypeError("‚ùå ERREUR : `all_words` contient encore des dictionnaires.")

# Cr√©ation des labels
labels = [1] * len(sca_words) + [0] * len(non_sca_words)

# Tokenisation des mots-cl√©s
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_words)  # ‚úÖ Maintenant `all_words` est une liste de cha√Ænes

sequences = tokenizer.texts_to_sequences(all_words)
X = pad_sequences(sequences)

# S√©parer en donn√©es d'entra√Ænement et validation
X_train, X_val, y_train, y_val = train_test_split(X, labels, test_size=0.2, random_state=42)

# Construire le mod√®le
input_shape = (X_train.shape[1],)
print("input_shape", input_shape)
model = build_mlp_model(input_shape, num_classes=2)

# Entra√Æner le mod√®le
model.fit(X_train, np.array(y_train), validation_data=(X_val, np.array(y_val)), epochs=10, batch_size=8)

# Sauvegarder le mod√®le entra√Æn√©
model_save_path = "models/sca_model.h5"
model.save(model_save_path)

# V√©rifier si le fichier a bien √©t√© cr√©√©
import os
if os.path.exists(model_save_path):
    print(f"‚úÖ Mod√®le entra√Æn√© et sauvegard√© avec succ√®s √† {model_save_path}")
else:
    print("‚ùå Erreur : Le mod√®le n'a pas √©t√© sauvegard√© correctement.")
